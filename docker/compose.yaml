# Default compose file - uses KRaft mode (Kafka without Zookeeper)
# Usage: docker compose -f docker/compose.yaml up -d
# Configuration:
#   1. Copy .env.example to .env: cp .env.example .env
#   2. Customize .env with your settings (optional - defaults are production-ready)
#   3. Run docker compose - it will automatically load .env
#
# Architecture:
#   Coinbase WS → ingest → Kafka (ticks.raw) → featurizer → Kafka (ticks.features) →
#   prediction-consumer → API /predict → Prometheus → Grafana
#   pr-auc-monitor → MLflow (tracks PR-AUC over time)
#
# Services:
#   - Infrastructure: kafka, mlflow
#   - API: api (FastAPI service)
#   - Pipeline: ingest, featurizer, prediction-consumer
#   - Monitoring: prometheus, grafana, pr-auc-monitor
#   - Init: kafka-init (one-time topic creation)

services:
  # Kafka broker (KRaft mode - no Zookeeper needed)
  # Provides event streaming platform for the pipeline
  # Topics: ticks.raw (raw ticker data), ticks.features (computed features)
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    ports:
      - "9092:9092"  # PLAINTEXT_HOST listener (external access)
      - "9093:9093"  # Controller port for KRaft
      - "29092:29092"  # PLAINTEXT listener (internal Docker network)
    environment:
      # KRaft mode configuration
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,CONTROLLER://0.0.0.0:9093,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # Kafka settings
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      # Cluster ID required for KRaft mode (Confluent image)
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - crypto-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s  # Give Kafka more time to start

  # MLflow Tracking Server
  # Used for experiment tracking during model training
  # Models are served from filesystem (not directly from MLflow)
  # Access UI at http://localhost:5001
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.2
    container_name: mlflow-server
    ports:
      - "5001:5000"
    volumes:
      # Mount local directory for MLflow data and artifacts
      # Persists experiment data across container restarts
      - ./mlflow_data:/mlflow
    command: >
      mlflow server
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --artifacts-destination /mlflow/artifacts
      --default-artifact-root http://localhost:5001/api/2.0/mlflow-artifacts/artifacts/experiments
      --host 0.0.0.0
      --port 5000
      --serve-artifacts
    networks:
      - crypto-network

  # FastAPI Service - Volatility Prediction API
  # Endpoints: POST /predict, GET /health, GET /version, GET /metrics
  # Loads models from filesystem at startup (not from MLflow)
  # Exposes Prometheus metrics for monitoring
  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile.api
    container_name: volatility-api
    ports:
      - "8000:8000"
    environment:
      # Model variant toggle: "ml" for trained model, "baseline" for z-score fallback
      # Change to "baseline" for rollback: MODEL_VARIANT=baseline
      - MODEL_VARIANT=${MODEL_VARIANT:-ml}
      # Model version: random_forest, logistic_regression, etc.
      - MODEL_VERSION=${MODEL_VERSION:-random_forest}
      # Model paths (loaded from filesystem, not MLflow)
      - MODEL_PATH=/app/models/artifacts/${MODEL_VERSION:-random_forest}/model.pkl
      - BASELINE_MODEL_PATH=/app/models/artifacts/baseline/model.pkl
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - GIT_SHA=${GIT_SHA:-dev}
    volumes:
      # Read-only mount: models loaded at startup
      - ../models:/app/models:ro
      # Read-only mount: data access for reference (if needed)
      - ../data:/app/data:ro
    depends_on:
      kafka:
        condition: service_healthy
      mlflow:
        condition: service_started
    networks:
      - crypto-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka Topic Initialization Service
  # One-time service that creates required Kafka topics
  # Runs once and exits (restart: "no")
  # Other services depend on this completing successfully
  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - crypto-network
    entrypoint: ['/bin/sh', '-c']
    command: |
      "
      echo 'Waiting for Kafka to be ready...'
      sleep 15
      kafka-topics --create --if-not-exists --topic ticks.raw --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 || true
      kafka-topics --create --if-not-exists --topic ticks.features --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 || true
      kafka-topics --create --if-not-exists --topic predictions.log --bootstrap-server kafka:29092 --partitions 1 --replication-factor 1 || true
      echo 'Kafka topics initialized'
      "
    restart: "no"

  # WebSocket Ingestion Service
  # Connects to Coinbase Advanced Trade WebSocket
  # Streams real-time ticker data to Kafka ticks.raw topic
  # Optionally saves raw data to data/raw/ (persisted via volume)
  ingest:
    build:
      context: ..
      dockerfile: docker/Dockerfile.ingestor
    container_name: crypto-ingest
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_TOPIC_RAW=ticks.raw
      - COINBASE_WS_URL=${COINBASE_WS_URL:-wss://advanced-trade-ws.coinbase.com}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ../data/raw:/app/data/raw  # For --save-disk option
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    networks:
      - crypto-network
    restart: unless-stopped
    command: ["python", "scripts/ws_ingest.py", "--pair", "BTC-USD", "--save-disk"]

  # Feature Engineering Service
  # Consumes raw ticks from Kafka ticks.raw topic
  # Computes windowed features (30s, 60s, 300s windows)
  # Publishes features to Kafka ticks.features topic
  # Saves processed features to data/processed/ (persisted via volume)
  featurizer:
    build:
      context: ..
      dockerfile: docker/Dockerfile.ingestor
    container_name: crypto-featurizer
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - FEATURIZER_METRICS_PORT=8001
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ../data/processed:/app/data/processed
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      ingest:
        condition: service_started
    networks:
      - crypto-network
    restart: unless-stopped
    command: >
      python scripts/featurizer.py
      --topic_in ticks.raw
      --topic_out ticks.features
      --bootstrap_servers kafka:29092
      --output_file data/processed/features_live.parquet
      --windows 30 60 300
      --add-labels

  # Prediction Consumer Service
  # Consumes features from Kafka ticks.features topic
  # Formats features according to API contract
  # Automatically calls /predict API endpoint
  # Rate-limited to prevent API overload (120 requests/minute default)
  prediction-consumer:
    build:
      context: ..
      dockerfile: docker/Dockerfile.ingestor
    container_name: crypto-prediction-consumer
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_TOPIC_FEATURES=ticks.features
      - KAFKA_TOPIC_PREDICTIONS=predictions.log
      - API_BASE_URL=http://api:8000
      - PREDICTION_CONSUMER_METRICS_PORT=8002
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_PREDICTIONS=${LOG_PREDICTIONS:-true}
      - PREDICTIONS_LOG_DIR=/app/logs/predictions
      - MAX_REQUESTS_PER_MINUTE=${MAX_REQUESTS_PER_MINUTE:-600}  # 10/second for replay tests
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      api:
        condition: service_healthy
      featurizer:
        condition: service_started
    volumes:
      - ../models:/app/models:ro  # Access to model threshold metadata
      - ../logs/predictions:/app/logs/predictions  # Prediction logs for comparison
    networks:
      - crypto-network
    restart: unless-stopped
    command: ["python", "scripts/prediction_consumer.py"]

  # PR-AUC Monitoring Service
  # Continuously monitors model performance by calculating PR-AUC on live predictions
  # Compares predictions with labels from featurizer
  # Logs metrics to MLflow every 10 minutes for time-series visualization
  # Access charts at http://localhost:5001
  pr-auc-monitor:
    build:
      context: ..
      dockerfile: docker/Dockerfile.ingestor
    container_name: crypto-pr-auc-monitor
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_TOPIC_PREDICTIONS=predictions.log
      - PREDICTIONS_LOG_DIR=/app/logs/predictions
      - FEATURES_LIVE_PATH=/app/data/processed/features_live.parquet
      - PR_AUC_UPDATE_INTERVAL=60  # 1 minute
      - PR_AUC_TIME_WINDOW=0.5  # Last 0.5 hours (30 minutes)
      - PR_AUC_LOOKBACK_BUFFER=5  # Exclude last 5 minutes to ensure labels are available
      - KAFKA_CONSUMER_TIMEOUT_MS=30000  # 30 seconds (sufficient for recent data)
      - KAFKA_MAX_MESSAGES=50000  # Max messages to read per cycle
      - MLFLOW_EXPERIMENT=crypto-volatility-production
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    volumes:
      - ../models:/app/models:ro  # Access to model threshold metadata (if needed)
      - ../data/processed:/app/data/processed  # Access to features_live.parquet
      - ../logs/predictions:/app/logs/predictions  # Access to prediction logs
    depends_on:
      mlflow:
        condition: service_started
      featurizer:
        condition: service_started
      prediction-consumer:
        condition: service_started
    networks:
      - crypto-network
    restart: unless-stopped
    command: ["python", "scripts/monitor_pr_auc.py"]

  # Prometheus Metrics Database
  # Scrapes metrics from API /metrics endpoint
  # Stores time-series metrics for monitoring
  # Access UI at http://localhost:9090
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      # Prometheus configuration (scraping targets, alert rules)
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      # Persists metrics data across container restarts
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.enable-lifecycle'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - crypto-network
    depends_on:
      - api
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Grafana Dashboards
  # Visualizes Prometheus metrics with real-time dashboards
  # Pre-configured dashboards for latency, error rate, predictions
  # Access UI at http://localhost:3000 (admin/admin123)
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      # Enable anonymous access so users can view dashboards without login
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
      - GF_AUTH_ANONYMOUS_ORG_NAME=Main Org.
      # Allow Grafana to start even if datasource provisioning fails initially
      - GF_DATABASE_MAX_IDLE_CONN=2
      - GF_DATABASE_MAX_OPEN_CONN=0
      - GF_DATABASE_CONN_MAX_LIFETIME=14400
      # Disable strict provisioning validation (allows Grafana to start even if datasource fails)
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - crypto-network
    depends_on:
      prometheus:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

networks:
  crypto-network:
    driver: bridge

volumes:
  kafka-data:
  prometheus-data:
  grafana-data:

# Note: Kafka is running in KRaft mode (no Zookeeper needed)
# MLflow data is stored in ./mlflow_data (bind mount)
# API service loads models from ../models (read-only mount)
